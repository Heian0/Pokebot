{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PokeBot - A Deep Reinforcement Learning Research Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to study deep reinforcement learning techniques, specifically deep Q learning, by training a DQN agent to play competitive pokemon battles via the Pokemon Showdown battle simulator. \n",
    "\n",
    "There are a couple \"goals\" I hope our bot will be able to accomplish by the end of this project - while it would be very difficult/impossible to train a model actually comparable to a human player when it comes to random single battles (a popular competitive pokemon battle format on pokemon showdown) given our resources and huge domain space that comes with random battles, we want to see if it is possible for the bot to succeed in a smaller state space.\n",
    "\n",
    "Specifically, I want the agent to be able to beat Sinnoh Pokemon Champion Cynthia's famous team in the Diamond/Pearl/Platinum series with a standard Sinnoh playthrough team. This is of course a well known and notoriously difficult battle (even for human players) as Cynthia's team is incredibly rounded and full of strong Pokemon. Excellent strategy is normally required to beat her. However, it will be a bit easier for our bot as it will not be playing against the exact Cynthia program from the original games (I don't have access to that algorithm/bot), rather it will play against a MaxDamage agent, a relatively strong opponent that will give our bot a good challenge.\n",
    "\n",
    "I also want to take a deeper dive into the specific strategies that our bot ends up utilizing, and if they fall in line with what a real player might do, like switch in a water type against a fire type. The second goal for our agent will also be to pick up meta strategies will a little help in the form of giving specific rewards. The strategy in question involves one of my personal favourite Pokemon, Typhlosion.\n",
    "\n",
    "Choice Scarf Hisuian Typhlosion is a fun and meta viable (ish) Pokemon because of its ability to learn the move Overheat, an extremely powerful fire type attack that has max power when its user is at full health, and drops in power as the user's hp falls. I want to see if our bot can pick up on this, and use a different move when Typhlosion loses a lot of hp, even if it has been previously rewarded a lot for using Overheat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off I will begin by installing a couple of modules, including the poke-env module, which is an environment for training RL agents to play Pokemon Showdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "from battle import Battle\n",
    "from teams import OUR_TEAM, CYNTHIAS_TEAM, NAME_TO_ID_DICT\n",
    "from gymnasium.spaces import Box, Space\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from tabulate import tabulate\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from poke_env.player import (\n",
    "    Gen4EnvSinglePlayer,\n",
    "    Gen9EnvSinglePlayer,\n",
    "    MaxBasePowerPlayer,\n",
    "    RandomPlayer,\n",
    "    SimpleHeuristicsPlayer,\n",
    "    background_cross_evaluate,\n",
    "    background_evaluate_player,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need to set up the Pokemon Showdown API using Node to run a local server so we can see our agent in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd pokemon-showdown\n",
    "!node pokemon-showdown start --no-security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VS Champion Cynthia Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vs_cynthia_pokebot(Gen4EnvSinglePlayer):\n",
    "    def __init__(self, battle_format, opponent, team, start_challenging): # Make sure to pass in gen4ou as battle format vs Cynthia\n",
    "        super().__init__(battle_format=battle_format, opponent=opponent, team=team, start_challenging=start_challenging)\n",
    "        self.num_battles = 0\n",
    "        self._ACTION_SPACE = list(range(4 + 5))\n",
    "\n",
    "    def calc_reward(self, prev_state: Battle, \n",
    "                    curr_state: Battle, \n",
    "                    starting_value = 0.0, \n",
    "                    hp_value: float = 0.20, \n",
    "                    fainted_value: float = 0.20, \n",
    "                    team_size = 6, \n",
    "                    victory_reward = 1.0,\n",
    "                    slp_reward = 0.15,\n",
    "                    brn_reward = 0.10,\n",
    "                    par_reward = 0.10,\n",
    "                    tox_reward = 0.15) -> float:\n",
    "\n",
    "        if curr_state not in self._reward_buffer:\n",
    "            self._reward_buffer[curr_state] = starting_value\n",
    "        reward = 0\n",
    "\n",
    "        # Grab our active pokemon\n",
    "        active_pkmn = curr_state.active_pokemon\n",
    "\n",
    "        # Want to give a reward if we switch into a pokemon that can do supereffective STAB damage\n",
    "        good_type_matchup_reward = 0\n",
    "        our_pkmn_types = (active_pkmn.type_1, active_pkmn.type_2)\n",
    "\n",
    "        for our_type in our_pkmn_types:\n",
    "                if our_type:\n",
    "                    if curr_state.opponent_active_pokemon.damage_multiplier(our_type) == 2:\n",
    "                        good_type_matchup_reward += 0.5\n",
    "                    if curr_state.opponent_active_pokemon.damage_multiplier(our_type) == 4:\n",
    "                        good_type_matchup_reward += 1\n",
    "\n",
    "        reward += good_type_matchup_reward\n",
    "\n",
    "        # Want to give a reward if our stats have been raised\n",
    "        for boost in active_pkmn.boosts:\n",
    "            reward = reward + active_pkmn.boost[boost] * 0.25\n",
    "\n",
    "        # Reward for inflicting Sleep/Paralysis/Toxic/Burn\n",
    "        if curr_state.opponent_active_pokemon.status.SLP: reward += slp_reward\n",
    "        if curr_state.opponent_active_pokemon.status.BRN: reward += brn_reward\n",
    "        if curr_state.opponent_active_pokemon.status.PAR: reward += par_reward\n",
    "        if curr_state.opponent_active_pokemon.status.TOX: reward += tox_reward\n",
    "\n",
    "        # HP/Fainted rewards for our team\n",
    "        for pkmn in curr_state.team.values():\n",
    "            reward += pkmn.current_hp_fraction * hp_value\n",
    "            if pkmn.fainted:\n",
    "                reward -= fainted_value\n",
    "\n",
    "        reward += (team_size - len(curr_state.team)) * hp_value\n",
    "\n",
    "        # HP/Fainted rewards for defending team\n",
    "        for pkmn in curr_state.opponent_team.values():\n",
    "            reward -= pkmn.current_hp_fraction * hp_value\n",
    "            if pkmn.fainted:\n",
    "                reward += fainted_value\n",
    "\n",
    "        reward -= (team_size - len(curr_state.opponent_team)) * hp_value\n",
    "\n",
    "        # Win/Loss rewards\n",
    "        if curr_state.won:\n",
    "            reward += victory_reward\n",
    "        elif curr_state.lost:\n",
    "            reward -= victory_reward\n",
    "\n",
    "        # Value to return\n",
    "        self._reward_buffer[curr_state] = reward\n",
    "        return reward - self._reward_buffer[curr_state]\n",
    "\n",
    "    def embed_battle(self, battle: Battle):\n",
    "        # -1 indicates that the move does not have a base power or is not available\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = (\n",
    "                # Rescale to a smaller value between 1.0 and 2.0 for training\n",
    "                move.base_power / 100\n",
    "            )\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = battle.opponent_active_pokemon.damage_multiplier(move)\n",
    "                if move.type == battle.active_pokemon.type_1 or move.type == battle.active_pokemon.type_2:\n",
    "                    moves_dmg_multiplier[i] *= 1.5\n",
    "\n",
    "        # Counting how many pokemons have not fainted in each team\n",
    "        n_fainted_mon_team = (\n",
    "            len([mon for mon in battle.team.values() if mon.fainted])\n",
    "        )\n",
    "        n_fainted_mon_opponent = (\n",
    "            len([mon for mon in battle.opponent_team.values() if mon.fainted])\n",
    "        )\n",
    "\n",
    "        state= np.concatenate([\n",
    "            [NAME_TO_ID_DICT[str(battle.active_pokemon).split(' ')[0]]],\n",
    "            [NAME_TO_ID_DICT[str(battle.opponent_active_pokemon).split(' ')[0]]],\n",
    "            [move_base_power for move_base_power in moves_base_power],\n",
    "            [move_dmg_multiplier for move_dmg_multiplier in moves_dmg_multiplier],\n",
    "            [n_fainted_mon_team,\n",
    "            n_fainted_mon_opponent],\n",
    "            ])\n",
    "        \n",
    "        np.reshape(state,(1, 12))\n",
    "        print(state.shape)\n",
    "        return np.float32(state)\n",
    "\n",
    "    def describe_embedding(self) -> Space:\n",
    "        minimum = [0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0]\n",
    "        maximum = [5, 5, 2.5, 2.5, 2.5, 2.5, 4, 4, 4, 4, 6, 6]\n",
    "        return Box(\n",
    "            np.array(minimum, dtype=np.float32),\n",
    "            np.array(maximum, dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is contained in the pokebots.py file, but I have pasted it here just for easier viewing instead of importing it. It is an RL training environment that extends the Gen4EnvSinglePlayer environment from poke-env.\n",
    "\n",
    "Unlike the previous environment, this environment adds some new reward funtions aimed at improving our bots strategic play - namely switiching into a Pokemon that can sucessfully counter the defending Pokemon, and rewards using status moves heavily. It is also much easier to embed the state, as we have a grand total of only 12 possible Pokemon.\n",
    "\n",
    "We also want to define the CPU our agent will play against. Let's use the max damage player to give our agent a good challenege - Cynthia's team is full of heavy hitters, so a max damage strategy would be a good way to play with her team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poke_env import AccountConfiguration, ServerConfiguration\n",
    "from poke_env.teambuilder.teambuilder import Teambuilder\n",
    "\n",
    "\n",
    "class MaxDamagePlayer(RandomPlayer):\n",
    "    def choose_move(self, battle):\n",
    "\n",
    "        if battle.available_moves:\n",
    "            best_move = max(battle.available_moves, key=lambda move: move.base_power)\n",
    "            return self.create_order(best_move)\n",
    "\n",
    "        else:\n",
    "            return self.choose_random_move(battle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Q-Learning - Keras**\n",
    "\n",
    "The first method we have implemented was a **value-based** method: [Deep Q-Learning (DQN), from Keras-RL Agents (2018)](github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py).\n",
    "\n",
    "* Agent performs actions in the environment to learn how to map the observed states to certain actions;\n",
    "* The agent chooses an action in a given state based on a \"Q value\" (weighted reward based on the highest expected long-term reward);\n",
    "* DQN agent learns to perform its task in such a way that the recommended action maximizes potential future rewards;\n",
    "* The method is considered an \"Off-Policy\" method because its Q values are updated assuming the best action was chosen, even if the best action was not chosen.\n",
    "*  Q-value is calculated with the reward added to the next state maximum Q-value. Every time the Q-value calculates a high number for a certain state, the value that is obtained from the output of the neural network for that specific state, will become higher every time. Each output neuron value will get higher and higher until the difference between each output value is high;\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(\n",
    "            EpsGreedyQPolicy(),\n",
    "            attr=\"eps\",\n",
    "            value_max=1.0,\n",
    "            value_min=0.05,\n",
    "            value_test=0,\n",
    "            nb_steps=100000,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"int\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m N_STATE_COMPONENTS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m \u001b[38;5;66;03m# 12 dimensions describing our state\u001b[39;00m\n\u001b[1;32m      3\u001b[0m n_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m \u001b[38;5;66;03m# 9 possible actions\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_STATE_COMPONENTS\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(N_HIDDEN, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melu\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39minput_shape))\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"int\") to tuple"
     ]
    }
   ],
   "source": [
    "N_HIDDEN = 128 # Hidden layer count\n",
    "N_STATE_COMPONENTS = 12 # 12 dimensions describing our state\n",
    "n_action = 9 # 9 possible actions\n",
    "input_shape = (1,) + N_STATE_COMPONENTS\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, activation=\"elu\", input_shape=(1, 1, 12))\n",
    "# Our embedding has shape (1, 12), which affects our hidden layer dimension and output dimension\n",
    "# Flattening resolve potential issues that would arise otherwise\n",
    "model.add(Flatten())\n",
    "model.add(Dense(int(N_HIDDEN/2), activation=\"elu\"))\n",
    "model.add(Dense(n_action, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 19:22:01,123 - MaxDamagePlayer 1 - ERROR - Unhandled exception raised while handling message:\n",
      ">battle-gen4anythinggoes-114\n",
      "|request|{\"active\":[{\"moves\":[{\"move\":\"Embargo\",\"id\":\"embargo\",\"pp\":24,\"maxpp\":24,\"target\":\"normal\",\"disabled\":false},{\"move\":\"Silver Wind\",\"id\":\"silverwind\",\"pp\":8,\"maxpp\":8,\"target\":\"normal\",\"disabled\":false},{\"move\":\"Dark Pulse\",\"id\":\"darkpulse\",\"pp\":24,\"maxpp\":24,\"target\":\"any\",\"disabled\":false},{\"move\":\"Psychic\",\"id\":\"psychic\",\"pp\":16,\"maxpp\":16,\"target\":\"normal\",\"disabled\":false}]}],\"side\":{\"name\":\"MaxDamagePlayer 1\",\"id\":\"p2\",\"pokemon\":[{\"ident\":\"p2: Spiritomb\",\"details\":\"Spiritomb, L61, F\",\"condition\":\"164/164\",\"active\":true,\"stats\":{\"atk\":132,\"def\":172,\"spa\":147,\"spd\":188,\"spe\":76},\"moves\":[\"embargo\",\"silverwind\",\"darkpulse\",\"psychic\"],\"baseAbility\":\"pressure\",\"item\":\"\",\"pokeball\":\"pokeball\"},{\"ident\":\"p2: Gastrodon\",\"details\":\"Gastrodon, L60, M\",\"condition\":\"233/233\",\"active\":false,\"stats\":{\"atk\":135,\"def\":117,\"spa\":160,\"spd\":132,\"spe\":80},\"moves\":[\"muddywater\",\"sludgebomb\",\"stoneedge\",\"earthquake\"],\"baseAbility\":\"stickyhold\",\"item\":\"\",\"pokeball\":\"pokeball\"},{\"ident\":\"p2: Roserade\",\"details\":\"Roserade, L60, M\",\"condition\":\"176/176\",\"active\":false,\"stats\":{\"atk\":89,\"def\":92,\"spa\":205,\"spd\":163,\"spe\":137},\"moves\":[\"sludgebomb\",\"energyball\",\"shadowball\",\"extrasensory\"],\"baseAbility\":\"naturalcure\",\"item\":\"\",\"pokeball\":\"pokeball\"},{\"ident\":\"p2: Milotic\",\"details\":\"Milotic, L62, M\",\"condition\":\"221/221\",\"active\":false,\"stats\":{\"atk\":124,\"def\":137,\"spa\":163,\"spd\":170,\"spe\":134},\"moves\":[\"aquaring\",\"mirrorcoat\",\"icebeam\",\"surf\"],\"baseAbility\":\"marvelscale\",\"item\":\"\",\"pokeball\":\"pokeball\"},{\"ident\":\"p2: Garchomp\",\"details\":\"Garchomp, L66, F\",\"condition\":\"252/252\",\"active\":false,\"stats\":{\"atk\":210,\"def\":163,\"spa\":131,\"spd\":152,\"spe\":192},\"moves\":[\"dragonrush\",\"brickbreak\",\"earthquake\",\"gigaimpact\"],\"baseAbility\":\"sandveil\",\"item\":\"sitrusberry\",\"pokeball\":\"pokeball\"},{\"ident\":\"p2: Lucario\",\"details\":\"Lucario, L63, M\",\"condition\":\"199/199\",\"active\":false,\"stats\":{\"atk\":174,\"def\":125,\"spa\":202,\"spd\":111,\"spe\":149},\"moves\":[\"aurasphere\",\"dragonpulse\",\"psychic\",\"earthquake\"],\"baseAbility\":\"steadfast\",\"item\":\"\",\"pokeball\":\"pokeball\"}]},\"rqid\":3}\n",
      "|sentchoice|default\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/timothyli3360/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/poke_env/ps_client/ps_client.py\", line 135, in _handle_message\n",
      "    await self._handle_battle_message(split_messages)  # type: ignore\n",
      "  File \"/home/timothyli3360/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/poke_env/player/player.py\", line 361, in _handle_battle_message\n",
      "    battle.parse_message(split_message)\n",
      "  File \"/home/timothyli3360/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/poke_env/environment/abstract_battle.py\", line 822, in parse_message\n",
      "    raise NotImplementedError(split_message)\n",
      "NotImplementedError: ['', 'sentchoice', 'default']\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-33' coro=<PSClient._handle_message() done, defined at /home/timothyli3360/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/poke_env/ps_client/ps_client.py:121> exception=NotImplementedError(['', 'sentchoice', 'default'])>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/timothyli3360/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/poke_env/ps_client/ps_client.py\", line 190, in _handle_message\n",
      "    raise exception\n",
      "  File \"/home/timothyli3360/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/poke_env/ps_client/ps_client.py\", line 135, in _handle_message\n",
      "    await self._handle_battle_message(split_messages)  # type: ignore\n",
      "  File \"/home/timothyli3360/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/poke_env/player/player.py\", line 361, in _handle_battle_message\n",
      "    battle.parse_message(split_message)\n",
      "  File \"/home/timothyli3360/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/poke_env/environment/abstract_battle.py\", line 822, in parse_message\n",
      "    raise NotImplementedError(split_message)\n",
      "NotImplementedError: ['', 'sentchoice', 'default']\n",
      "2024-07-09 19:22:01.224499: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-07-09 19:22:01.243288: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_2_1/bias/Assign' id:194 op device:{requested: '', assigned: ''} def:{{{node dense_2_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2_1/bias, dense_2_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,)\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "cynthia = MaxDamagePlayer(battle_format=\"gen4anythinggoes\", team=CYNTHIAS_TEAM)\n",
    "vs_cynthia_pokebot_train_env = vs_cynthia_pokebot(battle_format=\"gen4anythinggoes\", opponent=cynthia, team=OUR_TEAM, start_challenging=True)\n",
    "vs_cynthia_pokebot_test_env = vs_cynthia_pokebot(battle_format=\"gen4anythinggoes\", opponent=cynthia, team=OUR_TEAM, start_challenging=True)\n",
    "\n",
    "dqn = DQNAgent(\n",
    "        model=model,\n",
    "        nb_actions=9,\n",
    "        policy=policy,\n",
    "        memory=memory,\n",
    "        nb_steps_warmup=int(1000),\n",
    "        gamma=0.75,\n",
    "        target_model_update=1,\n",
    "        delta_clip=0.01,\n",
    "        enable_double_dqn=False\n",
    "    )\n",
    "dqn.compile(optimizer=Adam(learning_rate=2.5e-4), metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timothyli3360/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-07-09 19:22:01.475681: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_2/BiasAdd' id:95 op device:{requested: '', assigned: ''} def:{{{node dense_2/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_2/MatMul, dense_2/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-07-09 19:22:01.492818: W tensorflow/c/c_api.cc:305] Operation '{name:'total_2/Assign' id:310 op device:{requested: '', assigned: ''} def:{{{node total_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_2, total_2/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[1] = 1 is not in [0, 1)\n\t [[{{node dense/Tensordot/GatherV2}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvs_cynthia_pokebot_train_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m vs_cynthia_pokebot_train_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/rl/core.py:168\u001b[0m, in \u001b[0;36mAgent.fit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    165\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_step_begin(episode_step)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# This is were all of the work happens. We first perceive and compute the action\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# (forward step) and then use the reward to improve (backward step).\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mprocess_action(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/rl/agents/dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# Select an action.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mget_recent_state(observation)\n\u001b[0;32m--> 224\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    226\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mselect_action(q_values\u001b[38;5;241m=\u001b[39mq_values)\n",
      "File \u001b[0;32m~/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/rl/agents/dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_q_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 68\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_batch_q_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions,)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[0;32m~/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/rl/agents/dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_batch_q_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_batch):\n\u001b[1;32m     62\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[0;32m---> 63\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mlen\u001b[39m(state_batch), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_actions)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_values\n",
      "File \u001b[0;32m~/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/keras/src/engine/training_v1.py:1321\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(inputs)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_predict_function()\n\u001b[0;32m-> 1321\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/keras/src/backend.py:4607\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4599\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4603\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\n\u001b[1;32m   4604\u001b[0m ):\n\u001b[1;32m   4605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4607\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches) :])\n\u001b[1;32m   4609\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4611\u001b[0m     fetched[: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[1;32m   4612\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   4613\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pokebot-env/lib/python3.9/site-packages/tensorflow/python/client/session.py:1505\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1504\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1505\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1508\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1509\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[1] = 1 is not in [0, 1)\n\t [[{{node dense/Tensordot/GatherV2}}]]"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "dqn.fit(vs_cynthia_pokebot_train_env, nb_steps=100000, verbose=1)\n",
    "vs_cynthia_pokebot_train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "print(\"Results against random player:\")\n",
    "dqn.test(vs_cynthia_pokebot_test_env, nb_episodes=100, verbose=False, visualize=False)\n",
    "print(\n",
    "    f\"DQN Evaluation: {vs_cynthia_pokebot_test_env.n_won_battles} victories out of {vs_cynthia_pokebot_test_env.n_finished_battles} episodes\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokebot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
